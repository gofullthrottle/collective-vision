# Epic 2.1: AI Infrastructure Foundation

## Methodology Guidance
**SPECTRA Phase**: Implementation/AI Core
**Approach**: Build foundational AI services for embeddings, LLM, vectors, and async processing
**Tools**: Cloudflare Workers AI, Vectorize, Queues, Claude API

## Wave Context
**Wave**: 2 - AI Infrastructure + P0 Capabilities
**Priority**: Critical (enables all AI features)
**Dependencies**: Epic 1.2 (auth for API access)
**Estimated Duration**: 10 hours

## Quality Requirements
- Embeddings generated in < 500ms
- LLM responses properly structured
- Queue processing handles failures gracefully
- Usage tracking per workspace for billing

---

## Tasks

### 2.1.1 Embedding Service Setup (2h)
**Objective**: Generate text embeddings using Cloudflare Workers AI

**Steps**:
1. Configure Workers AI binding in wrangler.toml:
   ```toml
   [ai]
   binding = "AI"
   ```

2. Create embedding utility module:
   ```typescript
   // src/lib/ai/embeddings.ts

   export async function generateEmbedding(
     ai: Ai,
     text: string
   ): Promise<number[]> {
     // Normalize text
     const normalized = text.trim().toLowerCase().substring(0, 8000);

     const result = await ai.run('@cf/baai/bge-base-en-v1.5', {
       text: [normalized]
     });

     if (!result.data?.[0]) {
       throw new Error('Failed to generate embedding');
     }

     return result.data[0];
   }

   export async function generateEmbeddings(
     ai: Ai,
     texts: string[]
   ): Promise<number[][]> {
     const normalized = texts.map(t => t.trim().toLowerCase().substring(0, 8000));

     const result = await ai.run('@cf/baai/bge-base-en-v1.5', {
       text: normalized
     });

     return result.data;
   }
   ```

3. Add caching layer for repeated texts:
   ```typescript
   export async function getEmbeddingCached(
     ai: Ai,
     kv: KVNamespace,
     text: string
   ): Promise<number[]> {
     const cacheKey = `embed:${hashText(text)}`;
     const cached = await kv.get(cacheKey, 'json');

     if (cached) return cached as number[];

     const embedding = await generateEmbedding(ai, text);
     await kv.put(cacheKey, JSON.stringify(embedding), { expirationTtl: 86400 * 7 });

     return embedding;
   }
   ```

**Acceptance Criteria**:
- [ ] Embeddings generated for any text input
- [ ] 768-dimension vectors returned (bge-base-en-v1.5)
- [ ] Errors handled gracefully with retry
- [ ] Response time < 500ms for single embedding

---

### 2.1.2 Vector Store Setup (2h)
**Objective**: Configure Cloudflare Vectorize for similarity search

**Steps**:
1. Create Vectorize index:
   ```bash
   wrangler vectorize create feedback-embeddings --dimensions=768 --metric=cosine
   ```

2. Add binding to wrangler.toml:
   ```toml
   [[vectorize]]
   binding = "VECTORIZE"
   index_name = "feedback-embeddings"
   ```

3. Create vector operations utility:
   ```typescript
   // src/lib/ai/vectors.ts

   interface VectorMetadata {
     feedback_id: string;
     board_id: string;
     workspace_id: string;
     created_at: string;
   }

   export async function upsertVector(
     vectorize: VectorizeIndex,
     id: string,
     embedding: number[],
     metadata: VectorMetadata
   ): Promise<void> {
     await vectorize.upsert([{
       id,
       values: embedding,
       metadata
     }]);
   }

   export async function queryVectors(
     vectorize: VectorizeIndex,
     embedding: number[],
     options: {
       topK: number;
       filter?: Record<string, string>;
     }
   ): Promise<VectorizeMatches> {
     return vectorize.query(embedding, {
       topK: options.topK,
       filter: options.filter,
       returnMetadata: true
     });
   }

   export async function deleteVector(
     vectorize: VectorizeIndex,
     id: string
   ): Promise<void> {
     await vectorize.deleteByIds([id]);
   }
   ```

**Acceptance Criteria**:
- [ ] Vectors stored with metadata
- [ ] Similarity queries return scored matches
- [ ] Filtering by workspace/board works
- [ ] Delete removes vectors

---

### 2.1.3 LLM Integration for Classification (2h)
**Objective**: Integrate Claude API for structured classification tasks

**Steps**:
1. Configure Claude API environment:
   ```bash
   wrangler secret put ANTHROPIC_API_KEY
   ```

2. Create LLM utility module:
   ```typescript
   // src/lib/ai/llm.ts

   interface ClassificationResult {
     type: 'bug' | 'feature_request' | 'improvement' | 'question' | 'praise' | 'complaint';
     product_area: string | null;
     urgency: 'normal' | 'urgent' | 'critical';
     confidence: number;
     reasoning: string;
   }

   const CLASSIFICATION_PROMPT = `Classify this user feedback. Return ONLY valid JSON:

   {
     "type": "bug|feature_request|improvement|question|praise|complaint",
     "product_area": "inferred area or null",
     "urgency": "normal|urgent|critical",
     "confidence": 0.0-1.0,
     "reasoning": "brief explanation"
   }

   Feedback:
   Title: {title}
   Description: {description}`;

   export async function classifyFeedback(
     title: string,
     description: string,
     env: Env
   ): Promise<ClassificationResult> {
     const prompt = CLASSIFICATION_PROMPT
       .replace('{title}', title)
       .replace('{description}', description || 'No description');

     const response = await fetch('https://api.anthropic.com/v1/messages', {
       method: 'POST',
       headers: {
         'Content-Type': 'application/json',
         'x-api-key': env.ANTHROPIC_API_KEY,
         'anthropic-version': '2023-06-01'
       },
       body: JSON.stringify({
         model: 'claude-3-5-haiku-20241022',
         max_tokens: 256,
         messages: [{ role: 'user', content: prompt }]
       })
     });

     if (!response.ok) {
       throw new Error(`LLM API error: ${response.status}`);
     }

     const result = await response.json();
     const content = result.content[0].text;

     return JSON.parse(content) as ClassificationResult;
   }
   ```

3. Add retry logic with exponential backoff:
   ```typescript
   export async function classifyWithRetry(
     title: string,
     description: string,
     env: Env,
     maxRetries = 3
   ): Promise<ClassificationResult | null> {
     for (let attempt = 0; attempt < maxRetries; attempt++) {
       try {
         return await classifyFeedback(title, description, env);
       } catch (error) {
         if (attempt === maxRetries - 1) return null;
         await new Promise(r => setTimeout(r, Math.pow(2, attempt) * 1000));
       }
     }
     return null;
   }
   ```

**Acceptance Criteria**:
- [ ] Claude API calls working
- [ ] Structured JSON responses parsed correctly
- [ ] Retry with backoff on transient failures
- [ ] Fallback returns null (not error)

---

### 2.1.4 AI Processing Queue (2h)
**Objective**: Async job processing for AI tasks

**Steps**:
1. Create Cloudflare Queue:
   ```bash
   wrangler queues create ai-processing
   ```

2. Add bindings to wrangler.toml:
   ```toml
   [[queues.producers]]
   binding = "AI_QUEUE"
   queue = "ai-processing"

   [[queues.consumers]]
   queue = "ai-processing"
   max_batch_size = 10
   max_retries = 3
   dead_letter_queue = "ai-processing-dlq"
   ```

3. Define job types and producer:
   ```typescript
   // src/lib/ai/queue.ts

   type AIJobType = 'embed' | 'classify' | 'sentiment' | 'theme';

   interface AIJob {
     feedbackId: string;
     workspaceId: string;
     types: AIJobType[];
     attempt: number;
   }

   export async function enqueueAIJob(
     queue: Queue<AIJob>,
     feedbackId: string,
     workspaceId: string,
     types: AIJobType[]
   ): Promise<void> {
     await queue.send({
       feedbackId,
       workspaceId,
       types,
       attempt: 1
     });
   }
   ```

4. Create queue consumer:
   ```typescript
   // src/workers/ai-processor.ts

   export default {
     async queue(
       batch: MessageBatch<AIJob>,
       env: Env
     ): Promise<void> {
       for (const message of batch.messages) {
         const job = message.body;

         try {
           await processAIJob(job, env);
           message.ack();
         } catch (error) {
           if (job.attempt >= 3) {
             console.error(`Job failed permanently: ${job.feedbackId}`);
             message.ack(); // Send to DLQ
           } else {
             message.retry({ delaySeconds: Math.pow(2, job.attempt) * 60 });
           }
         }
       }
     }
   };
   ```

**Acceptance Criteria**:
- [ ] Jobs enqueued and processed asynchronously
- [ ] Failed jobs retried with exponential backoff
- [ ] Dead letter queue captures permanent failures
- [ ] Batch processing supported

---

### 2.1.5 Cost Tracking & Limits (2h)
**Objective**: Track AI usage per workspace for billing and limits

**Steps**:
1. Create usage tracking table:
   ```sql
   CREATE TABLE ai_usage (
     id TEXT PRIMARY KEY,
     workspace_id TEXT NOT NULL,
     date TEXT NOT NULL,  -- YYYY-MM-DD
     embeddings_count INTEGER DEFAULT 0,
     llm_calls_count INTEGER DEFAULT 0,
     vector_queries_count INTEGER DEFAULT 0,
     created_at TEXT DEFAULT (datetime('now')),
     updated_at TEXT DEFAULT (datetime('now')),
     UNIQUE(workspace_id, date),
     FOREIGN KEY (workspace_id) REFERENCES workspaces(id)
   );
   ```

2. Create usage tracking utility:
   ```typescript
   // src/lib/ai/usage.ts

   export async function incrementUsage(
     db: D1Database,
     workspaceId: string,
     field: 'embeddings' | 'llm_calls' | 'vector_queries'
   ): Promise<void> {
     const today = new Date().toISOString().split('T')[0];
     const columnMap = {
       embeddings: 'embeddings_count',
       llm_calls: 'llm_calls_count',
       vector_queries: 'vector_queries_count'
     };

     await db.prepare(`
       INSERT INTO ai_usage (id, workspace_id, date, ${columnMap[field]})
       VALUES (?, ?, ?, 1)
       ON CONFLICT(workspace_id, date) DO UPDATE SET
         ${columnMap[field]} = ${columnMap[field]} + 1,
         updated_at = datetime('now')
     `).bind(generateId('usage'), workspaceId, today).run();
   }

   export async function checkUsageLimit(
     db: D1Database,
     workspaceId: string,
     tier: 'free' | 'pro' | 'enterprise'
   ): Promise<{ allowed: boolean; usage: number; limit: number }> {
     const limits = {
       free: { embeddings: 1000, llm_calls: 100 },
       pro: { embeddings: 50000, llm_calls: 5000 },
       enterprise: { embeddings: -1, llm_calls: -1 }  // Unlimited
     };

     const today = new Date().toISOString().split('T')[0];
     const usage = await db.prepare(`
       SELECT SUM(embeddings_count) as embeddings, SUM(llm_calls_count) as llm_calls
       FROM ai_usage
       WHERE workspace_id = ? AND date >= date(?, '-30 days')
     `).bind(workspaceId, today).first();

     const tierLimits = limits[tier];
     const embeddingsUsed = usage?.embeddings || 0;

     return {
       allowed: tierLimits.embeddings === -1 || embeddingsUsed < tierLimits.embeddings,
       usage: embeddingsUsed,
       limit: tierLimits.embeddings
     };
   }
   ```

3. Add usage endpoint:
   ```typescript
   // GET /api/v1/workspaces/:id/ai-usage
   async function handleGetAIUsage(request: Request, env: Env) {
     const { workspaceId } = parseParams(request);
     await requirePermission(request, env, workspaceId, 'analytics:view');

     const usage = await getMonthlyUsage(workspaceId, env);
     const limits = await getWorkspaceLimits(workspaceId, env);

     return jsonResponse({
       current_month: usage,
       limits,
       percentage_used: {
         embeddings: (usage.embeddings / limits.embeddings) * 100,
         llm_calls: (usage.llm_calls / limits.llm_calls) * 100
       }
     });
   }
   ```

**Acceptance Criteria**:
- [ ] Usage tracked per day per workspace
- [ ] Tier limits enforced before AI operations
- [ ] Usage visible in admin dashboard
- [ ] Alerts when approaching limits (80%)

---

## Definition of Done
- [ ] Embedding service generating 768-dim vectors
- [ ] Vectorize storing and querying vectors
- [ ] Claude API integration working
- [ ] Queue processing AI jobs asynchronously
- [ ] Usage tracking and limits enforced
- [ ] All services integrated and tested

## Technical Notes
- Use Claude 3.5 Haiku for cost-effective classification
- Cache embeddings in KV for 7 days
- Vectorize has 10M vector limit per index
- Queue retry delays: 1min, 2min, 4min

## Related Files
- `src/lib/ai/embeddings.ts` - Embedding generation
- `src/lib/ai/vectors.ts` - Vectorize operations
- `src/lib/ai/llm.ts` - Claude API integration
- `src/lib/ai/queue.ts` - Job queue utilities
- `src/lib/ai/usage.ts` - Usage tracking
- `src/workers/ai-processor.ts` - Queue consumer
