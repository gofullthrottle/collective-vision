# Epic 5.1: Import Infrastructure

## Methodology Guidance
**SPECTRA Phase**: Implementation/Data
**Approach**: Build robust data import pipeline
**Tools**: Cloudflare Queues, D1, R2, deduplication

## Wave Context
**Wave**: 5 - Data Ingestion & Multi-Channel
**Priority**: P1 (competitive migration path)
**Dependencies**: Wave 2 (AI pipeline), Wave 0 (schema)
**Estimated Duration**: 6 hours

## Quality Requirements
- Graceful handling of large imports (10k+ items)
- Progress tracking with resume capability
- Duplicate detection before insert
- Full rollback on critical errors

---

## Tasks

### 5.1.1 Import Job Management (2h)
**Objective**: Track import progress and state

**Steps**:
1. Create import jobs table:
   ```sql
   CREATE TABLE import_jobs (
     id TEXT PRIMARY KEY,
     workspace_id TEXT NOT NULL,
     source_type TEXT NOT NULL,  -- 'uservoice', 'canny', 'csv', etc.
     status TEXT DEFAULT 'pending',  -- pending, processing, completed, failed, cancelled
     total_items INTEGER DEFAULT 0,
     processed_items INTEGER DEFAULT 0,
     imported_items INTEGER DEFAULT 0,
     skipped_items INTEGER DEFAULT 0,
     failed_items INTEGER DEFAULT 0,
     error_log TEXT,  -- JSON array of errors
     config TEXT,     -- JSON import configuration
     started_at TEXT,
     completed_at TEXT,
     created_by TEXT NOT NULL,
     created_at TEXT DEFAULT (datetime('now')),
     FOREIGN KEY (workspace_id) REFERENCES workspaces(id),
     FOREIGN KEY (created_by) REFERENCES users(id)
   );

   CREATE INDEX idx_import_jobs_workspace ON import_jobs(workspace_id, status);
   CREATE INDEX idx_import_jobs_status ON import_jobs(status, created_at);
   ```

2. Create import job API:
   ```typescript
   // POST /api/v1/workspaces/:id/imports
   interface CreateImportRequest {
     source_type: 'uservoice' | 'canny' | 'productboard' | 'csv' | 'json';
     config: {
       board_id?: string;           // Target board
       default_status?: string;     // Default status for imported items
       merge_duplicates?: boolean;  // Merge or skip duplicates
       dry_run?: boolean;           // Preview without importing
     };
     file_id?: string;  // R2 file reference for CSV/JSON
     api_credentials?: {
       api_key?: string;
       subdomain?: string;
     };
   }

   async function handleCreateImport(request: Request, env: Env): Promise<Response> {
     const { workspaceId } = parseParams(request);
     const input = await validateBody(request, CreateImportSchema);

     await requirePermission(request, env, workspaceId, 'imports:create');

     const jobId = generateId('imp');

     await env.DB.prepare(`
       INSERT INTO import_jobs (id, workspace_id, source_type, config, created_by)
       VALUES (?, ?, ?, ?, ?)
     `).bind(
       jobId,
       workspaceId,
       input.source_type,
       JSON.stringify(input.config),
       request.user.id
     ).run();

     // Queue the import job
     await env.IMPORT_QUEUE.send({
       type: 'start_import',
       job_id: jobId,
       workspace_id: workspaceId,
       source_type: input.source_type,
       config: input.config,
       file_id: input.file_id,
       api_credentials: input.api_credentials
     });

     return jsonResponse({ job_id: jobId, status: 'pending' }, 202);
   }
   ```

3. Create import progress endpoint:
   ```typescript
   // GET /api/v1/workspaces/:id/imports/:jobId
   async function handleGetImportStatus(request: Request, env: Env): Promise<Response> {
     const { workspaceId, jobId } = parseParams(request);

     await requirePermission(request, env, workspaceId, 'imports:read');

     const job = await env.DB.prepare(`
       SELECT * FROM import_jobs WHERE id = ? AND workspace_id = ?
     `).bind(jobId, workspaceId).first();

     if (!job) {
       return errorResponse('NOT_FOUND', 'Import job not found', 404);
     }

     const progress = job.total_items > 0
       ? Math.round((job.processed_items / job.total_items) * 100)
       : 0;

     return jsonResponse({
       id: job.id,
       status: job.status,
       progress,
       stats: {
         total: job.total_items,
         processed: job.processed_items,
         imported: job.imported_items,
         skipped: job.skipped_items,
         failed: job.failed_items
       },
       errors: job.error_log ? JSON.parse(job.error_log) : [],
       started_at: job.started_at,
       completed_at: job.completed_at
     });
   }
   ```

4. Create import queue consumer:
   ```typescript
   // src/queues/import-consumer.ts
   export async function handleImportQueue(
     batch: MessageBatch,
     env: Env
   ): Promise<void> {
     for (const message of batch.messages) {
       const { type, job_id, workspace_id, source_type, config, file_id, api_credentials } = message.body;

       try {
         if (type === 'start_import') {
           await startImport(env, job_id, workspace_id, source_type, config, file_id, api_credentials);
         } else if (type === 'process_batch') {
           await processBatch(env, message.body);
         }
         message.ack();
       } catch (error) {
         console.error(`Import error for job ${job_id}:`, error);

         await env.DB.prepare(`
           UPDATE import_jobs
           SET status = 'failed',
               error_log = json_insert(COALESCE(error_log, '[]'), '$[#]', ?),
               completed_at = datetime('now')
           WHERE id = ?
         `).bind(JSON.stringify({ error: error.message, timestamp: new Date().toISOString() }), job_id).run();

         message.ack();  // Don't retry failed jobs
       }
     }
   }

   async function startImport(
     env: Env,
     jobId: string,
     workspaceId: string,
     sourceType: string,
     config: ImportConfig,
     fileId?: string,
     apiCredentials?: ApiCredentials
   ): Promise<void> {
     // Update status to processing
     await env.DB.prepare(`
       UPDATE import_jobs SET status = 'processing', started_at = datetime('now')
       WHERE id = ?
     `).bind(jobId).run();

     // Get items to import based on source
     let items: ImportItem[];

     switch (sourceType) {
       case 'csv':
       case 'json':
         items = await parseFileImport(env, fileId!, sourceType);
         break;
       case 'uservoice':
         items = await fetchUserVoiceItems(apiCredentials!);
         break;
       case 'canny':
         items = await fetchCannyItems(apiCredentials!);
         break;
       case 'productboard':
         items = await fetchProductboardItems(apiCredentials!);
         break;
       default:
         throw new Error(`Unknown source type: ${sourceType}`);
     }

     // Update total count
     await env.DB.prepare(`
       UPDATE import_jobs SET total_items = ? WHERE id = ?
     `).bind(items.length, jobId).run();

     // Queue batches for processing
     const BATCH_SIZE = 50;
     for (let i = 0; i < items.length; i += BATCH_SIZE) {
       await env.IMPORT_QUEUE.send({
         type: 'process_batch',
         job_id: jobId,
         workspace_id: workspaceId,
         config,
         items: items.slice(i, i + BATCH_SIZE),
         batch_index: Math.floor(i / BATCH_SIZE)
       });
     }
   }
   ```

**Acceptance Criteria**:
- [ ] Jobs track progress accurately
- [ ] Status updates in real-time
- [ ] Large imports processed in batches
- [ ] Failed items don't block import

---

### 5.1.2 Duplicate Detection (2h)
**Objective**: Prevent duplicate imports

**Steps**:
1. Create import signatures table:
   ```sql
   CREATE TABLE import_signatures (
     id TEXT PRIMARY KEY,
     workspace_id TEXT NOT NULL,
     signature_hash TEXT NOT NULL,  -- SHA-256 of normalized content
     feedback_id TEXT NOT NULL,
     source_type TEXT NOT NULL,
     source_id TEXT,               -- Original ID from source platform
     imported_at TEXT DEFAULT (datetime('now')),
     FOREIGN KEY (workspace_id) REFERENCES workspaces(id),
     FOREIGN KEY (feedback_id) REFERENCES feedback_items(id)
   );

   CREATE UNIQUE INDEX idx_import_sig_unique ON import_signatures(workspace_id, signature_hash);
   CREATE INDEX idx_import_sig_source ON import_signatures(workspace_id, source_type, source_id);
   ```

2. Implement signature generation:
   ```typescript
   // src/lib/import/signatures.ts

   function normalizeText(text: string): string {
     return text
       .toLowerCase()
       .replace(/\s+/g, ' ')
       .replace(/[^\w\s]/g, '')
       .trim();
   }

   export async function generateSignature(item: ImportItem): Promise<string> {
     const normalized = normalizeText(item.title + ' ' + (item.description || ''));
     const encoder = new TextEncoder();
     const data = encoder.encode(normalized);
     const hashBuffer = await crypto.subtle.digest('SHA-256', data);
     const hashArray = Array.from(new Uint8Array(hashBuffer));
     return hashArray.map(b => b.toString(16).padStart(2, '0')).join('');
   }

   export async function checkDuplicate(
     env: Env,
     workspaceId: string,
     signatureHash: string
   ): Promise<{ isDuplicate: boolean; existingId?: string }> {
     const existing = await env.DB.prepare(`
       SELECT feedback_id FROM import_signatures
       WHERE workspace_id = ? AND signature_hash = ?
     `).bind(workspaceId, signatureHash).first();

     return {
       isDuplicate: !!existing,
       existingId: existing?.feedback_id as string | undefined
     };
   }

   export async function checkSourceDuplicate(
     env: Env,
     workspaceId: string,
     sourceType: string,
     sourceId: string
   ): Promise<{ isDuplicate: boolean; existingId?: string }> {
     const existing = await env.DB.prepare(`
       SELECT feedback_id FROM import_signatures
       WHERE workspace_id = ? AND source_type = ? AND source_id = ?
     `).bind(workspaceId, sourceType, sourceId).first();

     return {
       isDuplicate: !!existing,
       existingId: existing?.feedback_id as string | undefined
     };
   }
   ```

3. Implement semantic duplicate detection:
   ```typescript
   // src/lib/import/semantic-dedup.ts

   export async function findSemanticDuplicates(
     env: Env,
     workspaceId: string,
     title: string,
     description: string,
     threshold: number = 0.85
   ): Promise<Array<{ id: string; title: string; similarity: number }>> {
     // Generate embedding for new item
     const text = `${title} ${description || ''}`.slice(0, 1000);
     const embedding = await env.AI.run('@cf/baai/bge-base-en-v1.5', { text: [text] });

     // Search for similar items
     const results = await env.VECTORIZE.query(embedding.data[0], {
       topK: 5,
       namespace: `feedback_${workspaceId}`,
       returnMetadata: true
     });

     return results.matches
       .filter(m => m.score >= threshold)
       .map(m => ({
         id: m.id,
         title: m.metadata?.title as string,
         similarity: m.score
       }));
   }
   ```

4. Integrate into batch processing:
   ```typescript
   async function processBatch(env: Env, batch: BatchMessage): Promise<void> {
     const { job_id, workspace_id, config, items } = batch;

     let imported = 0;
     let skipped = 0;
     let failed = 0;
     const errors: string[] = [];

     for (const item of items) {
       try {
         // Check source ID duplicate first (fast)
         if (item.source_id) {
           const sourceCheck = await checkSourceDuplicate(
             env, workspace_id, item.source_type, item.source_id
           );
           if (sourceCheck.isDuplicate) {
             skipped++;
             continue;
           }
         }

         // Check content signature
         const signature = await generateSignature(item);
         const sigCheck = await checkDuplicate(env, workspace_id, signature);

         if (sigCheck.isDuplicate) {
           if (config.merge_duplicates && sigCheck.existingId) {
             await mergeWithExisting(env, sigCheck.existingId, item);
             imported++;
           } else {
             skipped++;
           }
           continue;
         }

         // Check semantic duplicates if enabled
         if (config.semantic_dedup) {
           const semanticDups = await findSemanticDuplicates(
             env, workspace_id, item.title, item.description, 0.9
           );
           if (semanticDups.length > 0) {
             if (config.merge_duplicates) {
               await mergeWithExisting(env, semanticDups[0].id, item);
               imported++;
             } else {
               skipped++;
             }
             continue;
           }
         }

         // Import as new item
         await importNewItem(env, workspace_id, config, item, signature);
         imported++;

       } catch (error) {
         failed++;
         errors.push(`Item "${item.title?.slice(0, 50)}": ${error.message}`);
       }
     }

     // Update job progress
     await env.DB.prepare(`
       UPDATE import_jobs
       SET processed_items = processed_items + ?,
           imported_items = imported_items + ?,
           skipped_items = skipped_items + ?,
           failed_items = failed_items + ?,
           error_log = CASE
             WHEN ? != '[]' THEN json_patch(COALESCE(error_log, '[]'), ?)
             ELSE error_log
           END
       WHERE id = ?
     `).bind(
       items.length,
       imported,
       skipped,
       failed,
       JSON.stringify(errors),
       JSON.stringify(errors),
       job_id
     ).run();

     // Check if job is complete
     const job = await env.DB.prepare(`
       SELECT total_items, processed_items FROM import_jobs WHERE id = ?
     `).bind(job_id).first();

     if (job && job.processed_items >= job.total_items) {
       await env.DB.prepare(`
         UPDATE import_jobs SET status = 'completed', completed_at = datetime('now')
         WHERE id = ?
       `).bind(job_id).run();
     }
   }
   ```

**Acceptance Criteria**:
- [ ] Source ID dedup prevents re-importing same item
- [ ] Content signature catches identical text
- [ ] Semantic dedup finds similar items
- [ ] Merge preserves vote counts

---

### 5.1.3 Source Attribution (1h)
**Objective**: Track where feedback originated

**Steps**:
1. Extend feedback_items schema:
   ```sql
   ALTER TABLE feedback_items ADD COLUMN source_type TEXT DEFAULT 'widget';
   ALTER TABLE feedback_items ADD COLUMN source_id TEXT;
   ALTER TABLE feedback_items ADD COLUMN source_url TEXT;
   ALTER TABLE feedback_items ADD COLUMN source_metadata TEXT;  -- JSON
   ALTER TABLE feedback_items ADD COLUMN imported_at TEXT;
   ALTER TABLE feedback_items ADD COLUMN import_job_id TEXT REFERENCES import_jobs(id);
   ```

2. Create source type constants:
   ```typescript
   // src/lib/import/sources.ts

   export const IMPORT_SOURCES = {
     widget: { name: 'Widget', icon: 'widget', color: '#3B82F6' },
     api: { name: 'API', icon: 'api', color: '#10B981' },
     mcp: { name: 'MCP Agent', icon: 'robot', color: '#8B5CF6' },
     uservoice: { name: 'UserVoice', icon: 'uservoice', color: '#FF6B00' },
     canny: { name: 'Canny', icon: 'canny', color: '#5D5FEF' },
     productboard: { name: 'Productboard', icon: 'productboard', color: '#6366F1' },
     csv: { name: 'CSV Import', icon: 'file', color: '#6B7280' },
     reddit: { name: 'Reddit', icon: 'reddit', color: '#FF4500' },
     discord: { name: 'Discord', icon: 'discord', color: '#5865F2' },
     slack: { name: 'Slack', icon: 'slack', color: '#4A154B' },
     twitter: { name: 'Twitter/X', icon: 'twitter', color: '#1DA1F2' },
     support: { name: 'Support Ticket', icon: 'ticket', color: '#F59E0B' },
     email: { name: 'Email', icon: 'mail', color: '#EF4444' },
     web_scrape: { name: 'Web Scrape', icon: 'globe', color: '#06B6D4' }
   } as const;

   export type ImportSource = keyof typeof IMPORT_SOURCES;
   ```

3. Implement source attribution in import:
   ```typescript
   async function importNewItem(
     env: Env,
     workspaceId: string,
     config: ImportConfig,
     item: ImportItem,
     signatureHash: string
   ): Promise<string> {
     const feedbackId = generateId('fb');
     const now = new Date().toISOString();

     // Create feedback item with source attribution
     await env.DB.prepare(`
       INSERT INTO feedback_items (
         id, board_id, title, description, status,
         source_type, source_id, source_url, source_metadata,
         imported_at, import_job_id,
         moderation_state, is_hidden, created_at
       ) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
     `).bind(
       feedbackId,
       config.board_id,
       item.title,
       item.description,
       config.default_status || 'new',
       item.source_type,
       item.source_id,
       item.source_url,
       JSON.stringify(item.source_metadata || {}),
       now,
       config.job_id,
       config.auto_approve ? 'approved' : 'pending',
       config.auto_approve ? 0 : 1,
       item.created_at || now
     ).run();

     // Record import signature
     await env.DB.prepare(`
       INSERT INTO import_signatures (id, workspace_id, signature_hash, feedback_id, source_type, source_id)
       VALUES (?, ?, ?, ?, ?, ?)
     `).bind(
       generateId('sig'),
       workspaceId,
       signatureHash,
       feedbackId,
       item.source_type,
       item.source_id
     ).run();

     // Import votes if present
     if (item.votes && item.votes > 0) {
       await env.DB.prepare(`
         INSERT INTO feedback_votes (id, feedback_id, user_id, weight, created_at)
         VALUES (?, ?, ?, ?, ?)
       `).bind(
         generateId('vote'),
         feedbackId,
         'imported_votes',  // Special user for imported votes
         item.votes,
         now
       ).run();
     }

     // Import comments if present
     if (item.comments && item.comments.length > 0) {
       for (const comment of item.comments) {
         await env.DB.prepare(`
           INSERT INTO feedback_comments (id, feedback_id, user_id, content, is_internal, created_at)
           VALUES (?, ?, ?, ?, ?, ?)
         `).bind(
           generateId('cmt'),
           feedbackId,
           'imported_comment',
           comment.content,
           comment.is_internal ? 1 : 0,
           comment.created_at || now
         ).run();
       }
     }

     // Queue for AI processing
     await env.AI_QUEUE.send({
       type: 'process_feedback',
       feedback_id: feedbackId,
       workspace_id: workspaceId
     });

     return feedbackId;
   }
   ```

**Acceptance Criteria**:
- [ ] Source type tracked on all items
- [ ] Original IDs preserved
- [ ] Source URLs linkable
- [ ] Metadata searchable

---

### 5.1.4 Import Rollback (1h)
**Objective**: Undo imports when needed

**Steps**:
1. Create rollback endpoint:
   ```typescript
   // DELETE /api/v1/workspaces/:id/imports/:jobId
   async function handleRollbackImport(request: Request, env: Env): Promise<Response> {
     const { workspaceId, jobId } = parseParams(request);

     await requirePermission(request, env, workspaceId, 'imports:delete');

     // Verify job exists and belongs to workspace
     const job = await env.DB.prepare(`
       SELECT * FROM import_jobs WHERE id = ? AND workspace_id = ?
     `).bind(jobId, workspaceId).first();

     if (!job) {
       return errorResponse('NOT_FOUND', 'Import job not found', 404);
     }

     if (job.status === 'processing') {
       return errorResponse('CONFLICT', 'Cannot rollback in-progress import', 409);
     }

     // Start rollback in background
     await env.IMPORT_QUEUE.send({
       type: 'rollback_import',
       job_id: jobId,
       workspace_id: workspaceId
     });

     return jsonResponse({ status: 'rolling_back' }, 202);
   }
   ```

2. Implement rollback logic:
   ```typescript
   async function rollbackImport(env: Env, jobId: string, workspaceId: string): Promise<void> {
     // Update job status
     await env.DB.prepare(`
       UPDATE import_jobs SET status = 'rolling_back' WHERE id = ?
     `).bind(jobId).run();

     // Get all feedback IDs from this import
     const feedbackItems = await env.DB.prepare(`
       SELECT id FROM feedback_items WHERE import_job_id = ?
     `).bind(jobId).all();

     const feedbackIds = feedbackItems.results.map(r => r.id as string);

     if (feedbackIds.length === 0) {
       await env.DB.prepare(`
         UPDATE import_jobs SET status = 'rolled_back', completed_at = datetime('now')
         WHERE id = ?
       `).bind(jobId).run();
       return;
     }

     // Delete in order (respect foreign keys)
     const placeholders = feedbackIds.map(() => '?').join(',');

     // Delete votes
     await env.DB.prepare(`
       DELETE FROM feedback_votes WHERE feedback_id IN (${placeholders})
     `).bind(...feedbackIds).run();

     // Delete comments
     await env.DB.prepare(`
       DELETE FROM feedback_comments WHERE feedback_id IN (${placeholders})
     `).bind(...feedbackIds).run();

     // Delete tags
     await env.DB.prepare(`
       DELETE FROM feedback_item_tags WHERE feedback_id IN (${placeholders})
     `).bind(...feedbackIds).run();

     // Delete from vector index
     for (const id of feedbackIds) {
       try {
         await env.VECTORIZE.deleteByIds([id]);
       } catch (e) {
         console.error(`Failed to delete vector for ${id}:`, e);
       }
     }

     // Delete import signatures
     await env.DB.prepare(`
       DELETE FROM import_signatures WHERE feedback_id IN (${placeholders})
     `).bind(...feedbackIds).run();

     // Delete feedback items
     await env.DB.prepare(`
       DELETE FROM feedback_items WHERE id IN (${placeholders})
     `).bind(...feedbackIds).run();

     // Update job status
     await env.DB.prepare(`
       UPDATE import_jobs
       SET status = 'rolled_back',
           completed_at = datetime('now'),
           imported_items = 0
       WHERE id = ?
     `).bind(jobId).run();
   }
   ```

3. Add rollback confirmation UI helper:
   ```typescript
   // GET /api/v1/workspaces/:id/imports/:jobId/preview-rollback
   async function handlePreviewRollback(request: Request, env: Env): Promise<Response> {
     const { workspaceId, jobId } = parseParams(request);

     await requirePermission(request, env, workspaceId, 'imports:read');

     const stats = await env.DB.prepare(`
       SELECT
         COUNT(*) as feedback_count,
         COALESCE(SUM((SELECT COUNT(*) FROM feedback_votes WHERE feedback_id = fi.id)), 0) as vote_count,
         COALESCE(SUM((SELECT COUNT(*) FROM feedback_comments WHERE feedback_id = fi.id)), 0) as comment_count
       FROM feedback_items fi
       WHERE import_job_id = ?
     `).bind(jobId).first();

     return jsonResponse({
       will_delete: {
         feedback_items: stats?.feedback_count || 0,
         votes: stats?.vote_count || 0,
         comments: stats?.comment_count || 0
       },
       warning: 'This action cannot be undone. All imported items and their associated data will be permanently deleted.'
     });
   }
   ```

**Acceptance Criteria**:
- [ ] Full rollback deletes all imported data
- [ ] Vectors cleaned from Vectorize
- [ ] Signatures removed
- [ ] Job marked as rolled_back

---

## Definition of Done
- [ ] Import jobs tracked with progress
- [ ] Duplicate detection working
- [ ] Source attribution complete
- [ ] Rollback functional
- [ ] Large imports handled efficiently

## Technical Notes
- Queue batch size: 50 items
- Signature uses SHA-256
- Semantic threshold: 0.85 similarity
- Rollback is destructive (no soft delete)

## Related Files
- `src/queues/import-consumer.ts` - Queue handler
- `src/lib/import/signatures.ts` - Deduplication
- `src/lib/import/sources.ts` - Source types
- `src/routes/imports.ts` - API endpoints
